---
title: Data testing - Test EndToEnd sur une pipeline data
date: 2023-01-18
---

## Objectifs :

+ Dans quel context et Pourquoi faut-il les mettre en place ?
  + Si vous apportez une modification d'infrastructure ou de code √† un pipeline, comment savez-vous que vous n'avez rien cass√© ?
  + Les tests de non-r√©gression manuels sont long et certaines fois des erreurs nous √©chappent (l'erreur humaine)
  + 
+ Comment les mettre en place (th√©orie)?
  + Env isol√© ?
  + des donn√©es de test fix√©es et connues en input
  + des donn√©es d'assertions verifi√©es √† la main
  + Rapprochement du m√©tier/PO --> BDD
--> Comment les mettre en place (pratique) ? (futur article)


------------

<!--truncate-->



Les tests des pipelines de donn√©es sont diff√©rents des tests d'autres applications, comme le backend d'un site web.

Il y a une dualit√© de tests dans un contexte data, les tests sur la donn√©e et sur le code :
+ Les tests de qualit√© de donn√©es ont pour r√¥le de : 
  + D√©tecter des anomalies 
  + Signaler des valeurs de donn√©es aberrantes.
+ Les tests sur le code assurent la qualit√© logicielle minimale, ces diff√©rents types de tests sont repr√©sent√©s ci-desous
 
![img.png](static/data-testing/pyramide-test.png)
https://jaayap.github.io/Unity_Best_Practices/Fr/Unit_Test_And_TDD.html

## Table des mati√®res


* [Rappels : ](#rappels-)
* [Dans quel context ?](#dans-quel-context-)
* [Pourquoi faut-il les mettre en place ?](#pourquoi-faut-il-les-mettre-en-place-)
* [Par ou commencer ?](#par-ou-commencer-)
* [Covention Dalkia](#convention-dalkia)

------------------------------------------------------------------------------------------------------------------------

## Rappels : 

### C'est quoi une pipeline de donn√©e ?

![img.png](static/data-testing/data-pipeline.png)
> üí° Dans le livre [Fundamentals of Data Engineering](https://www.oreilly.com/library/view/fundamentals-of-data/9781098108298/) 
> il associe la pipeline de donn√©e au Data Engineering lifecycle, c'est l'endroit ou le Data Engineer intervient le plus
> souvent.

_Les donn√©es entrent en continues d'un c√¥t√© du pipeline, progressent √† travers une s√©rie d'√©tapes et sortent sous la forme 
de rapports, de mod√®les et de vues._   

_Il est utile de conceptualiser le pipeline de donn√©es comme une cha√Æne de fabrication o√π la qualit√©, l'efficacit√©, 
les contraintes et le temps de fonctionnement doivent √™tre g√©r√©s._

> Le pipeline de donn√©es est le c√¥t√© **"op√©rationnel"** de l'analyse des donn√©es.

### Les √©tapes d'une pipeline de donn√©e :

1. **G√©n√©ration / Syst√®me source :** c'est l'origine des donn√©es utilis√©es.  
   _Par exemple_, un syst√®me source peut √™tre : 
   * Un objet IOT
   * Une application qui g√®re une file d'attente de messages
   * Une base de donn√©e transactionnelle  
   
   Souvent en tant que Data Engineer, nous consommons des donn√©es d'un syst√®me source mais nous n'en n'avons pas le contr√¥le.

2. **Stockage :** Vous avez besoin d'un endroit pour stocker :
   *  Les donn√©es du syst√®me source 
   *  Les r√©sultats des transformations
   *  Les donn√©es expos√©es 
   
   Quel type de solution de stockage devez-vous utiliser ? Cela d√©pend de vos cas d'utilisation, des volumes de donn√©es,
   de la fr√©quence d'ingestion, du format et de la taille des donn√©es ing√©r√©es.

3. **Ingestion :** Apr√®s avoir compris la source de donn√©es, les caract√©ristiques du syst√®me source que vous utilisez et 
   la mani√®re dont les donn√©es sont stock√©es, `vous devez rassembler les donn√©es`. L'√©tape suivante du cycle de vie 
   du Data Engineering est l'ingestion des donn√©es √† partir des syst√®mes sources.   
   > üí° Pour cette √©tape il y 2 grands concepts √† connaitre que nous n'addresserons pas dans cet article: 
   * `Batch` VS `Streaming` 
   * `Push` VS `Pull` 
   
4. **Transformation :** Signifie que les donn√©es doivent √™tre chang√©es de leur forme originale en quelque chose d'`utile` pour
   les cas d'utilisation en aval.  
   Sans transformations appropri√©es, les donn√©es resteront inertes, et ne seront pas sous une forme utile pour les 
   rapports, l'analyse ou le ML. Typiquement, l'√©tape de transformation est celle o√π les donn√©es commencent √† cr√©er 
   de la `valeur` pour la consommation des utilisateurs en aval.

5. **Donn√©es de consommation :** Maintenant que les donn√©es ont √©t√© ing√©r√©es, stock√©es et transform√©es en structures 
   coh√©rentes et utiles, il est temps de les `valoriser`.  
   Les valoriser √† travers le utilisations les plus courantes comme :
   * L'`analytique` 
   * Le `ML` 
   * Le `reverse ETL`

Dans l'analyse de donn√©es, il y a deux fa√ßons courantes d'√™tre embarrass√© professionnellement :
*  Laisser des `donn√©es de mauvaise qualit√©` atteindre les utilisateurs.
*  D√©ployer des `changements qui cassent les syst√®mes de production`

Et pour √©viter ces probl√®mes nous allons introduire deux workflows cl√©s.

### Deux Workflows cl√©s : Le Pipeline de Valeur & Le Pipeline d'Innovation
> Ces 2 workflows cl√©s sont d√©finis dans le livre DataOps : https://dkproduction.wpenginepowered.com/wp-content/uploads/2020/11/DK_dataops_book_2nd_edition.pdf

L'analyse des donn√©es vise √† extraire de la valeur des donn√©es. C'est ce que nous appelons le pipeline de valeur. 
Le diagramme ci-dessous montre le pipeline de valeur progressant horizontalement de gauche √† droite. 
Les donn√©es entrent dans le pipeline et passent en traitement de production. La production repr√©sente la pipeline de donn√©e. 

Lorsque les donn√©es sortent du pipeline, sous la sous forme d'analyses utiles, de la `valeur est cr√©√©e pour l'organisation`.

Avant d'expliquer ces 2 workflows, regardons la diff√©rence entre une pipeline `CI/CD` dans un contexte `DevOps` ‚ö°Ô∏è `DataOps` :

![img.png](static/data-testing/dataops-vs-devops.png)

Il est important de noter que l'orchestration** appara√Æt deux fois dans le processus DataOps illustr√© dans la figure ci-dessus.

La premi√®re orchestration repr√©sente la **pipeline d'innovation**.
Cetter derni√®re cherche √† am√©liorer l'analyse des donn√©es en mettant en ≈ìuvre de nouvelles id√©es qui produisent des 
_"insigths"_ analytiques. 

> _Exemple :_  
> _Actuellement nous mettons √† disposition les donn√©es aggr√©g√©es √† la journ√©e, et on aimerait √©galement mettre √† 
disposition les donn√©es aggr√©g√©es par mois et par pays._

Pendant le d√©veloppement de cette pipeline, les donn√©es restent statiques mais le code changent afin de pouvoir tester 
l'algorithm sans avoir d'effets de bords provenant de la donn√©e.

Sur le sch√©ma ci-dessus, la deuxi√®me orchestration repr√©sente la **pipeline de valeur**.  
Elle est une copie de la pipeline d'innovation dans l'environnement de production.  
Cette pipeline de donn√©e traite les donn√©es de production qui sont mises √† jours √† differentes fr√©quences, mais
le code reste constant.

> ‚ö†Ô∏è Le pire sc√©nario dans cette pipeline de valeur est de recevoir des donn√©es de mauvaise qualit√©. C'est pour √ßa qu'il faut
impl√©ment√© des tests de donn√©es.



 
> üí° **L'orchestration est un composant logiciel qui contr√¥le l'ex√©cution d'une pipeline de donn√©e en g√©rant les exceptions.

![img.png](static/data-testing/value-pipeline-vs-d-innovation-pipeline.png)

Sur le sch√©ma ci-dessus est repr√©sent√© horizontallement la pipeline de valeur et verticalement la pipeline d'innovation.

Ce qui est important √† comprendre c'est pendant le d√©veloppement d'une nouvelle/modification d'√©tape dans une pipeline data, la
donn√©e est fix√©e / stable. Tandis que le code va √™tre modifier r√©guli√®rement durant cette phase pour arriver au
fonctionnement souhait√© et √† la qualit√© souhait√©e. Il est important durant cette phase de mettre en place des tests sur 
le code. Ces tests peuvent √™tre unitaire, int√©gration, e2e, s√©curit√©, charge ... Une fois que le d√©veloppement est termin√©
on passe en production, et √† ce moment le code est fix√© ("releas√©") tandis que la donn√©e est mise √† jour. C'est dans cette phase
qu'il est important de monitorer et tester la qualit√© de sa donn√©e.

![img.png](img.png)



